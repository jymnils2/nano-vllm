{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# üì¶ Paso 1: Instalar dependencias\n",
        "!pip install -q llama-cpp-python==0.2.67 gradio==4.26.0\n",
        "\n",
        "# üóÉÔ∏è Paso 2: Montar Google Drive (opcional)\n",
        "from google.colab import drive\n",
        "use_gdrive = input(\"¬øDeseas usar Google Drive para guardar modelos? (s/n): \").strip().lower() == 's'\n",
        "\n",
        "if use_gdrive:\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = \"/content/drive/MyDrive/ModelosLLM/\"\n",
        "else:\n",
        "    base_path = \"/content/modelos_llm/\"\n",
        "\n",
        "import os\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "# üì• Paso 3: Definir modelos disponibles\n",
        "MODELOS = {\n",
        "    \"TinyLlama 1.1B Chat (INT4)\": {\n",
        "        \"url\": \"https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \",\n",
        "        \"nombre\": \"tinyllama.gguf\"\n",
        "    },\n",
        "    \"Phi-2 (2.7B INT4)\": {\n",
        "        \"url\": \"https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf \",\n",
        "        \"nombre\": \"phi2.gguf\"\n",
        "    },\n",
        "    \"Mistral 7B Instruct (INT4)\": {\n",
        "        \"url\": \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf \",\n",
        "        \"nombre\": \"mistral.gguf\"\n",
        "    },\n",
        "    \"LLaMA2 7B Chat (INT4)\": {\n",
        "        \"url\": \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf \",\n",
        "        \"nombre\": \"llama2.gguf\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# üì¶ Paso 4: Men√∫ para elegir modelo y descargar\n",
        "import urllib.request\n",
        "\n",
        "print(\"Modelos disponibles:\")\n",
        "for i, k in enumerate(MODELOS):\n",
        "    print(f\"{i+1}. {k}\")\n",
        "\n",
        "opcion = int(input(\"Selecciona un modelo (n√∫mero): \")) - 1\n",
        "seleccion = list(MODELOS.keys())[opcion]\n",
        "modelo_url = MODELOS[seleccion][\"url\"]\n",
        "modelo_nombre = MODELOS[seleccion][\"nombre\"]\n",
        "modelo_path = os.path.join(base_path, modelo_nombre)\n",
        "\n",
        "# Descargar modelo si no existe\n",
        "if not os.path.exists(modelo_path):\n",
        "    print(f\"Descargando {seleccion}...\")\n",
        "    urllib.request.urlretrieve(modelo_url, modelo_path)\n",
        "else:\n",
        "    print(f\"Modelo ya descargado: {modelo_path}\")\n",
        "\n",
        "# ü§ñ Paso 5: Cargar modelo con llama-cpp\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=modelo_path,\n",
        "    n_threads=8,\n",
        "    n_ctx=2048,\n",
        "    n_batch=512,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# üß† Paso 6: Funci√≥n de respuesta + interfaz Gradio\n",
        "import gradio as gr\n",
        "\n",
        "def responder(mensaje, historia):\n",
        "    historia.append((\"üßë\", mensaje))\n",
        "    prompt = \"\\n\".join([f\"Usuario: {x[0]}\\nAsistente: {x[1]}\" for x in historia[:-1]])\n",
        "    prompt += f\"\\nUsuario: {mensaje}\\nAsistente:\"\n",
        "\n",
        "    respuesta = llm(prompt, max_tokens=512, stop=[\"Usuario:\", \"Asistente:\"])\n",
        "    texto_respuesta = respuesta[\"choices\"][0][\"text\"].strip()\n",
        "    historia[-1] = (mensaje, texto_respuesta)\n",
        "    return texto_respuesta, historia\n",
        "\n",
        "iface = gr.ChatInterface(\n",
        "    responder,\n",
        "    chatbot=gr.Chatbot(),\n",
        "    title=\"üß† Chat con LLM (Nano style)\",\n",
        "    description=f\"Modelo: {seleccion}\",\n",
        "    theme=\"soft\",\n",
        ")\n",
        "\n",
        "iface.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
